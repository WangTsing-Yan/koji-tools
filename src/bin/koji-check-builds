#!/usr/bin/python

import Queue
from functools import partial
import logging
import optparse
import os
import threading
import time

import dateutil
import koji as _koji  # koji declared using profile module in main()
from koji.util import md5_constructor
from koji.util import sha1_constructor
import rpm


logger = logging.getLogger('koji.checkbuilds')

# an event to indicate that the feeder thread is done
feeder_done = threading.Event()

# queue to hold actions in the pipeline
queue = Queue.Queue()


def main():
    global koji
    parser = optparse.OptionParser(usage='%prog [options]')
    parser.add_option('-p', '--profile', default='koji',
                      help='pick a profile')
    parser.add_option('-j', '--threads', default=5, type='int',
                      help='worker thread count')

    # verbosity options
    parser.add_option("-d", "--debug", action="store_true",
                      help="show debug output")
    parser.add_option("-v", "--verbose", action="store_true",
                      help="show verbose output")
    parser.add_option("-q", "--quiet", action="store_true", default=False,
                      help="run quietly")

    # build selection options
    parser.add_option("--buildid", help="Check specific build from ID or nvr")
    parser.add_option("--package", help="Check builds for this package")
    parser.add_option("--before",
                      help="Check builds built before this time")
    parser.add_option("--after",
                      help="Check builds built after this time")
    parser.add_option("--type", help="Check builds of this type.")
    parser.add_option("--owner", help="Check builds built by this owner")
    parser.add_option("--volume", help="Check builds by volume ID")
    parser.add_option("--prefix", help="Only check packages starting with this prefix")

    # options for what to check
    parser.add_option("--ignore-strays", action='store_true',
                      help="Ignore stray files")
    parser.add_option("--ignore-rpm-size", action='store_true',
                      help="Ignore rpm size")
    # ^ if an rpm has been signed in place, the sigmd5 will still match the db,
    #   but the size will likely change
    parser.add_option("--no-sums", action='store_true',
                      help="Don't validate checksums")

    opts, args = parser.parse_args()

    if args:
        parser.error('This command takes no arguments. See --help for options')

    koji = _koji.get_profile_module(opts.profile)

    logging.basicConfig()
    if opts.debug:
        logger.setLevel(logging.DEBUG)
    elif opts.quiet:
        logger.setLevel(logging.ERROR)
    elif opts.verbose:
        logger.setLevel(logging.INFO)
    else:
        logger.setLevel(logging.WARN)

    # start our threads
    fthread = threading.Thread(name='feeder', target=feeder_thread,
                               args=(opts, args))
    fthread.setDaemon(True)
    fthread.start()

    threads = []
    for i in range(opts.threads):
        wthread = threading.Thread(name='worker %i' % i, target=worker_thread,
                                   args=(opts,))
        wthread.setDaemon(True)
        wthread.start()
        threads.append(wthread)

    mthread = threading.Thread(name='monitor', target=monitor_thread,
                               args=(fthread, queue, threads))
    mthread.setDaemon(True)
    mthread.start()
    while mthread.isAlive():
        mthread.join(timeout=5)


def monitor_thread(fthread, queue, threads):
    """Monitor the other threads"""
    # this part is its own thread so that the main thread can handle ctrl-c

    # the builds thread will be the first to finish
    fthread.join()
    feeder_done.set()

    # wait for the queue to be empty
    queue.join()

    logger.info('Finished. Waiting for workers to stop.')
    for wthread in threads:
        wthread.join()


sessions = threading.local()


def get_session():
    '''Get session for this thread, create if needed'''
    try:
        return sessions.session
    except AttributeError:
        sessions.session = _get_session()
        return sessions.session


def _get_session():
    '''Get a new anonymous session'''
    session_opts = koji.grab_session_options(koji.config)
    session_opts['anon_retry'] = True
    return koji.ClientSession(koji.config.server, session_opts)


def feeder_thread(opts, args):
    '''Fetch builds and feed them into the queu'''
    for i, build in enumerate(get_builds(opts, args)):
        while queue.qsize() > 1000:
            # avoid overloading the queue
            logger.debug('feeder waiting. queue is large')
            time.sleep(5)
        checker = BuildChecker(build, opts)
        queue.put([checker.check, {}])
        logger.debug("%i: queueing build %s", i, build['nvr'])
    logger.info('%i builds queued', i)


def get_builds(options, args):
    '''Yield all requested builds'''
    session = get_session()
    chunksize = 10000
    opts = {}
    opts['queryOpts'] = {'order': 'build.id', 'offset': 0, 'limit': chunksize}
    for key in ('type', 'prefix'):
        value = getattr(options, key)
        if value is not None:
            opts[key] = value
    if options.package:
        try:
            opts['packageID'] = int(options.package)
        except ValueError:
            package = session.getPackageID(options.package)
            if package is None:
                raise ValueError('invalid package option')
            opts['packageID'] = package
    if options.owner:
        try:
            opts['userID'] = int(options.owner)
        except ValueError:
            user = session.getUser(options.owner)
            if user is None:
                raise ValueError("Invalid owner option")
            opts['userID'] = user['id']
    if options.volume:
        try:
            opts['volumeID'] = int(options.volume)
        except ValueError:
            volumes = session.listVolumes()
            volumeID = None
            for volume in volumes:
                if options.volume == volume['name']:
                    volumeID = volume['id']
            if volumeID is None:
                raise ValueError("Invalid volume option")
            opts['volumeID'] = volumeID
    for opt in ('before', 'after'):
        val = getattr(options, opt)
        if not val:
            continue
        try:
            ts = float(val)
            setattr(options, opt, ts)
            continue
        except ValueError:
            pass
        try:
            dt = dateutil.parser.parse(val)
            ts = time.mktime(dt.timetuple())
            setattr(options, opt, ts)
        except:
            raise ValueError("Invalid time specification: %s" % val)
    if options.before:
        opts['completeBefore'] = getattr(options, 'before')
    if options.after:
        opts['completeAfter'] = getattr(options, 'after')

    while True:
        chunk = session.listBuilds(**opts)
        if not chunk:
            break
        opts['queryOpts']['offset'] += chunksize
        for build in chunk:
            yield build


def worker_thread(opts):
    '''Handle tasks in queue'''
    while True:
        try:
            func, kwargs = queue.get(block=True, timeout=5)
        except Queue.Empty:
            if feeder_done.is_set():
                # is this enough?
                break
            continue
        func(**kwargs)
        queue.task_done()


class BuildChecker(object):

    def __init__(self, build, options):
        self.build = build
        self.options = options

    def check(self):
        '''Initial build check, plus queue deeper checks'''
        build = self.build
        state = koji.BUILD_STATES[build['state']]
        self.build_dir = koji.pathinfo.build(build)
        if state == 'BUILDING':
            # ignore these
            return
        elif state in ('FAILED', 'DELETED', 'CANCELED'):
            if not self.options.ignore_strays and os.path.isdir(self.build_dir):
                logger.warn('Stray build directory: %s (build is %s)',
                             self.build_dir, state)
            # don't check further for these
            return
        elif state == 'COMPLETE':
            if not os.path.isdir(self.build_dir):
                logger.warn('Build directory missing: %s', self.build_dir)
                return
        else:
            # should not happen
            raise ValueError('Build state is %s' % state)
        queue.put([self.check_files, {}])

    def check_files(self):
        self.check_rpms()
        self.check_archives()
        # TODO: other files?

    def get_rpms(self):
        '''Get rpms to check'''
        session = get_session()
        rpms = []
        for rpminfo in session.listRPMs(buildID=self.build['build_id']):
            fn = '%s/%s' % (self.build_dir, koji.pathinfo.rpm(rpminfo))
            rpminfo['_fn'] = fn
            if rpminfo['metadata_only']:
                if os.path.lexists(fn):
                    logger.warn('Metadata-only rpm is present: %s', fn)
                logger.debug('Skipping metadata-only rpm: %s', fn)
                continue
            rpms.append(rpminfo)
        self.rpms = rpms
        return rpms

    def check_rpms(self):
        self.get_rpms()
        for rpminfo in self.rpms:
            fn = rpminfo['_fn']
            try:
                st = os.stat(fn)
            except OSError as ex:
                if ex.errno == 2:
                    logger.warn('Missing rpm: %s', fn)
                    continue
                raise
            if not self.options.ignore_rpm_size and st.st_size != rpminfo['size']:
                logger.warn('Wrong size for: %s', fn)
                logger.warn('  db: %i, file: %i', rpminfo['size'], st.st_size)
            try:
                hdr = koji.get_rpm_header(fn)
                # ^NOTE: this call does not *verify*
            except Exception:
                logger.warn('Unable to read header for: %s', fn)
                continue
            sigmd5 = koji.hex_string(hdr[rpm.RPMTAG_SIGMD5])
            if rpminfo['payloadhash'] != sigmd5:
                logger.warn('Wrong sigmd5 for: %s', fn)
                logger.warn('  db: %s, file: %s', rpminfo['payloadhash'], sigmd5)
        self.check_rpm_sigs()
        if not self.options.no_sums:
            queue.put([self.verify_rpms, {}])

    def verify_rpms(self):
        '''Actually verify the embedded checksums'''
        # should we just combine this with the earlier header check?
        ts = rpm.TransactionSet()
        ts.setVSFlags(rpm._RPMVSF_NOSIGNATURES)
        for rpminfo in self.rpms:
            fn = rpminfo['_fn']
            self.verify_rpm(fn, ts)

    def verify_rpm(self, fn, ts):
        with open(fn, 'r') as fp:
            try:
                ts.hdrFromFdno(fp.fileno())
            except rpm.error as ex:
                logger.warn('Could not verify rpm %s: %s', fn, ex)
            except Exception as ex:
                logger.exception("Error verifying rpm: %s", fn)

    def check_rpm_sigs(self):
        '''Validate signature data on disc'''
        build = self.build
        session = get_session()
        sig_idx = {}
        session.multicall = True
        for rpminfo in self.rpms:
            session.queryRPMSigs(rpm_id=rpminfo['id'])
        for rpminfo, [sigs] in zip(self.rpms, session.multiCall(strict=True)):
            sigs = session.queryRPMSigs(rpm_id=rpminfo['id'])
            for sig in sigs:
                sig_idx.setdefault(sig['sigkey'], []).append([rpminfo, sig])
        logger.debug('Keys for %s: %s', build['nvr'], sig_idx.keys())
        for sigkey in sig_idx:
            cachedir = os.path.join(self.build_dir, 'data/sigcache/%s' % sigkey)
            if not os.path.isdir(cachedir):
                logger.warning("Signature cache dir missing: %s", cachedir)
                continue
            # else
            for rpminfo, sig in sig_idx[sigkey]:
                cachefile = os.path.join(self.build_dir, koji.pathinfo.sighdr(rpminfo, sigkey))
                if not os.path.isfile(cachefile):
                    logger.warn("Cached signature missing: %s", cachefile)
                    continue
                sighash = md5_constructor(file(cachefile).read()).hexdigest()
                if sighash != sig['sighash']:
                    logger.warn('Cached signature mismatch for %s\n'
                            '  db: %s, file:%s',
                            cachefile, sig['sighash'], sighash)
        signed_to_check = []
        for sigkey in sig_idx:
            signeddir = os.path.join(self.build_dir, 'data/signed/%s' % sigkey)
            if not os.path.isdir(signeddir):
                # ok - signed copies are temporary
                continue
            for rpminfo, sig in sig_idx[sigkey]:
                signed = os.path.join(self.build_dir, koji.pathinfo.signed(rpminfo, sigkey))
                if not os.path.exists(signed):
                    # still ok
                    continue
                signed_to_check.append(signed)
                # check that sig header matches
                hdr = koji.rip_rpm_sighdr(signed)
                sighash = md5_constructor(hdr).hexdigest()
                if sighash != sig['sighash']:
                    logger.warn('Signed copy sighdr mismatch: %s\n'
                            '  db: %s, file: %s',
                            signed, sig['sighash'], sighash)
        if not self.options.no_sums:
            ts = rpm.TransactionSet()
            ts.setVSFlags(rpm._RPMVSF_NOSIGNATURES)
            logger.debug('Verifying %i signed copies for %s',
                    len(signed_to_check), self.build['nvr'])
            for fn in signed_to_check:
                self.verify_rpm(fn, ts)

    def check_archives(self):
        build = self.build
        session = get_session()
        # first gather archives
        archives = {}
        for legacy in ['maven', 'win', 'image']:
            for archive in session.listArchives(buildID=build['build_id'], type=legacy):
                archives.setdefault(archive['id'], archive)
        for archive in session.listArchives(buildID=build['build_id']):
            archives.setdefault(archive['id'], archive)

        logger.debug('Found %i archives for %s', len(archives), self.build['nvr'])
        self.archives = []
        for archive in archives.values():
            if archive['btype'] == 'maven':
                fn = '%s/%s' % (koji.pathinfo.mavenbuild(build),
                        koji.pathinfo.mavenfile(archive))
            elif archive['btype'] == 'win':
                fn = '%s/%s' % (koji.pathinfo.winbuild(build),
                        koji.pathinfo.winfile(archive))
            elif archive['btype'] == 'image':
                fn = '%s/%s' % (koji.pathinfo.imagebuild(build),
                        archive['filename'])
            else:
                fn = '%s/%s' % (koji.pathinfo.typedir(build, archive['btype']),
                        archive['filename'])
            archive['_fn'] = fn
            if archive['metadata_only']:
                if os.path.lexists(fn):
                    logger.warn('Metadata-only rpm is present: %s', fn)
                logger.debug('Skipping metadata-only archive: %s', fn)
                continue
            self.archives.append(archive)
            try:
                st = os.stat(fn)
            except OSError as ex:
                if ex.errno == 2:
                    logger.warn('Missing archive: %s', fn)
                    continue
                raise
            if st.st_size != archive['size']:
                logger.warn('Wrong size for: %s', fn)
                logger.warn('  db: %i, file: %i', archive['size'], st.st_size)
        if not self.options.no_sums:
            queue.put([self.verify_archives, {}])

    def verify_archives(self):
        logger.debug('Checking %i archive checksums for %s', len(self.archives),
                self.build['nvr'])
        for archive in self.archives:
            self.verify_checksum(archive['_fn'], archive['checksum_type'],
                    archive['checksum'])

    def verify_checksum(self, fn, sumtype, expect):
        try:
            sumtype = koji.CHECKSUM_TYPES[sumtype]
        except KeyError:
            logger.error('Unknown sum type %s for %s', sumtype, fn)
            return
        if sumtype == 'md5':
            chk = md5_constructor()
        elif sumtype == 'sha1':
            chk = sha1_constructor()
        else:
            # XXX
            logger.error('Unsupported sum type %s for %s', sumtype, fn)
            return
        logger.debug('Checking %s for %s', sumtype, fn)
        with file(fn, 'r') as fp:
            chunks = iter(partial(fp.read, 819200), b'')
            [chk.update(b) for b in chunks]
        value = chk.hexdigest()
        if value != expect:
            logger.warn('Checksum mismatch (%s) for %s\n'
                    '  db: %s, file: %s',
                    sumtype, fn, expect, value)




if __name__ == '__main__':
    main()
